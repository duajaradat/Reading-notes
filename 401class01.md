## Pain and Suffering

---

**These are all painful.**

**Pain** is what happens. **Suffering**is the story that we layer on top of what happens. Drop the story, learn how to feel, and then handle the situation as best you can. Done. Continue living.

---

## A beginner's guide to Big O Notation

1- Big O notation is used in Computer Science to describe the performance or complexity of an alogrithm, sepcifically it describes the worst-case scenario, and can be used to describe the execution time required or the space used (in memory or on disk) by an alogrithm.


2- O(1) desribes an algo that will always execute in the same time (or space) regardless of the size of the input data set.


3- O(N) describes an algo whose performance will grow linearly an din direct proportion to the size of the input data set.


4- O(N2) represents an algo whose performance is directly proportional to the square of the size of the input data set. This is common wiht algo that involve nested iterations over the data set. Deeper nested iterations will result in O(N3), O(N4), etc.


5- O(2^N) denotes an algo whose growth doubles with each addition to the input data set. The growth curve of an O(2^N) function is exponential - starting off very shallow, then rising meteorically.

## Logarithms

Binary search is a technique used to search sorted data sets. It works by selecting the middle element of the data set, essentially the median, and compares it against a target value. If the values match, it will return success. 

If the target value is higher than the value of the probe element, it will take the upper half of the data set and perform the same operation against it. The same goes if the target value is lower and goes until it can no longer split the data set.
O(logN) the iterative halving of data sets described in the binary search example produces a growth curse that peaks at the beginning and slowly flattens otu as the size of the data set increse e.g. an input data set containing 10 items takes one second to complete, a data set containing 100 items takes two seconds, and a data set containing 1,000 items will take three seconds.


Doubling the size of the input data set has little effect on its growth as after a single iteration of the alog the data set will be halved and therefor on a par with an input data set half the size.

This makes algorithms like binary search extremely efficient when dealing with large data sets.

---

## Names and Values:

1- Mutable Aliasing - similar to pass by reference in JavaScript. This refers to list and dictionaries that can have more than one reference, and changing one will change all references.

2- Immutable Values - ints, floats, strings, tuples. These values cant mutate. If you change these, behind the scene a new item is made and the reference points to that.

3- rebinding - changing an immutable value by creating the new data and having the name point to that.

4- mutating - changing things like list or dictionaries. Actual mutations occur.

---

## Python Environment:

1- Look into pyenv - used to install python

2- pyenv-virtualenv - used to configure global tools

3- Install by: curl https://pyenv.run | bash .

4- Add following to .bashrc or .zshrc:

     export PATH="~/.pyenv/bin:$PATH"
     eval "$(pyenv init -)"
     eval "$(pyenv virtualenv-init -)"
 
5- Install python interepreter: pyenv install
        `VERSION_YOU_WOULD_LIKE_TO_INSTALL`

6- poetry - manages project dependencies, separate projects through virtual environments, build applications & libraries.

7- Look into the following libraries - Black for code formatting, mypy for static type checker, pre-commit.