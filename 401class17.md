# Web Scraping!

## What the heck is it?

Web scraping is a technique to automatically access and extract large amounts of information from a website, which can save a huge amount of time and effort.

Always make sure you read website's legal information on usage rights.

## Okay, so how do we do it?

The first thing that we need to do is to figure out where we can locate the links to the files we want to download inside the multiple levels of HTML tags.

Start by right clicking the webpage and selecting "inspect"

Once the console is brought up click this icon:

<img src="https://miro.medium.com/max/30/1*OBTSehekWVX6rSXUaibd1A.png">

Now click on the file you want. The console should show you something like this:

```
<a href=”data/nyct/turnstile/turnstile_180922.txt”>Saturday, September 22, 2018</a>
```

Now that we’ve identified the location of the links, let’s get started on coding!

### Enter this in python:

```
import requests
import urllib.request
import time
from bs4 import BeautifulSoup
```

Set the URL to our request:

```
url = 'http://web.mta.info/developers/turnstile.html'
response = requests.get(url)
```

This is very similar to javascript API calls

If the access was successful, you should see the following output:

<img src="https://miro.medium.com/max/414/1*fyqRGzG8IbhhjxF2Q5MU_Q.png">

### Make it readable for python with beautiful soup

```
soup = BeautifulSoup(response.text, “html.parser”)

# Find all the <a> tags like this

soup.findAll('a')

# Next, let’s extract the actual link that we want. Let’s test out the first link.

one_a_tag = soup.findAll(‘a’)[38]
link = one_a_tag[‘href’]

# This code saves the first text file, ‘data/nyct/turnstile/turnstile_180922.txt’ to our variable link.

# Now use our urllib.request library to download this file path to our computer. We provide request.urlretrieve with two parameters: file url and the filename. 

download_url = 'http://web.mta.info/developers/'+ link
urllib.request.urlretrieve(download_url,'./'+link[link.find('/turnstile_')+1:])

# To make sure we're not spamming the website make sure to use this:

time.sleep(1)

```

## Great now I know how to webscrape, is there a faster way to do this?

Yes! With a for loop you can scrape for all the <a> links at once

```
# Import libraries
import requests
import urllib.request
import time
from bs4 import BeautifulSoup

# Set the URL you want to webscrape from
url = 'http://web.mta.info/developers/turnstile.html'

# Connect to the URL
response = requests.get(url)

# Parse HTML and save to BeautifulSoup object¶
soup = BeautifulSoup(response.text, "html.parser")

# To download the whole data set, let's do a for loop through all a tags
line_count = 1 #variable to track what line you are on
for one_a_tag in soup.findAll('a'):  #'a' tags are for links
    if line_count >= 36: #code for text files starts at line 36
        link = one_a_tag['href']
        download_url = 'http://web.mta.info/developers/'+ link
        urllib.request.urlretrieve(download_url,'./'+link[link.find('/turnstile_')+1:]) 
        time.sleep(1) #pause the code for a sec
    #add 1 for next line
    line_count +=1
```